{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using LLMs to Explore the IUPHAR Guide to Pharmacology\n",
    "Experiment with LLMs for the IUPHAR/BPS Guide to Pharmacology"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import base64\n",
    "import logging\n",
    "import codecs\n",
    "import requests\n",
    "import psycopg2\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To log any errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(filename=\"query_errors.log\", level=logging.ERROR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Connect to database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "def connect_to_db():\n",
    "    try:\n",
    "        conn = psycopg2.connect(**db_config)\n",
    "        return conn\n",
    "    except Exception as e:\n",
    "        print(f\"Error connecting to the database: {e}\")\n",
    "        return None\n",
    "    \n",
    "# Password generation function\n",
    "def pwd():\n",
    "    s1 = ''.join([chr(int(i)) for i in ['120', '65', '103', '108', '101', '116', '116', '55']])\n",
    "    s2 = base64.b64encode(s1.encode('utf-8')).decode('utf-8')\n",
    "    s3 = codecs.encode(s2[::-1], 'rot_13')\n",
    "    s4 = codecs.decode(s3[::-1], 'rot_13')\n",
    "    return base64.b64decode(s4).decode('utf-8')\n",
    "\n",
    "\n",
    "# Database connection details\n",
    "db_config = {\n",
    "    'host': 'localhost',\n",
    "    'database': 'guide_to_pharmacology',\n",
    "    'user': 'postgres',\n",
    "    'password': pwd(),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to the database successfully.\n"
     ]
    }
   ],
   "source": [
    "conn = connect_to_db()\n",
    "if conn:\n",
    "    print(\"Connected to the database successfully.\")\n",
    "else:\n",
    "    print(\"Failed to connect to the database. Please check your credentials.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function to Train LLM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to convert the training set into OpenAI's prompt-completion format\n",
    "def convert_to_openai_format(df):\n",
    "    openai_format_data = []\n",
    "    for _, row in df.iterrows():\n",
    "        # Format each row as a prompt-completion pair\n",
    "        openai_format_data.append({\n",
    "            \"prompt\": f\"{row['question']}\",\n",
    "            \"completion\": row['sql_query']\n",
    "        })\n",
    "    return openai_format_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function to Integrate LLM and SQL training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to ask a question and get SQL using OpenAI API\n",
    "def ask_openai(question, train_df=None):\n",
    "    api_key = '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'\n",
    "    headers = {\n",
    "        \"Content-Type\": \"application/json\",\n",
    "        \"Authorization\": f\"Bearer {api_key}\"\n",
    "    }\n",
    "\n",
    "    # If a training set is provided, convert it to the OpenAI format and append it to the system prompt\n",
    "    if train_df is not None:\n",
    "        training_data = convert_to_openai_format(train_df)\n",
    "        examples = \"\\n\".join([f\"Q: {item['prompt']} A: {item['completion']}\" for item in training_data])\n",
    "    else:\n",
    "        examples = \"\"\n",
    "    \n",
    "    # Constructing the data payload for OpenAI API with additional examples from the training data\n",
    "    data = {\n",
    "        \"model\": \"gpt-4o\",  # Or use your fine-tuned model if available\n",
    "        \"messages\": [\n",
    "            {\"role\": \"system\", \"content\": f'''\n",
    "                I have a database with the following schema:\n",
    "                Table: ‘object’ with columns: (abbreviation, only_grac, in_cgtp, annotation_status,\n",
    "                comments, object_id, only_iuphar, cite_id, systematic_name, gtip_comment, name,\n",
    "                gtmp_comment, in_gtip, old_object_id, in_gtmp, grac_comments, last_modified,\n",
    "                no_contributor_list, structural_info_comments, quaternary_structure_comments).\n",
    "                \n",
    "                Table: ‘ligand’ with columns: (old_ligand_id, has_chembl_interaction,\n",
    "                name_vector, mechanism_of_action_vector, comments, immuno_comments, antibacterial,\n",
    "                ligand_id, bioactivity_comments, absorption_distribution_vector, pubchem_sid,\n",
    "                metabolism_vector, comments_vector, popn_pharmacokinetics, bioactivity_comments_vector, elimination, drugs_url, abbreviation,\n",
    "                absorption_distribution, organ_function_impairment, emc_url, verified, mechanism_of_action, approved_source, who_essential, abbreviation_vector,\n",
    "                in_gtmp, metabolism, type, elimination_vector, approved, in_gtip, labelled, withdrawn_drug,\n",
    "                name, immuno_comments_vector, popn_pharmacokinetics_vector, ema_url, iupac_name,\n",
    "                clinical_use_vector, gtmp_comments, radioactive, has_qi_interaction,\n",
    "                gtmp_comments_vector, organ_function_impairment_vector, clinical_use).\n",
    "                \n",
    "                Table: ‘interaction’ with columns: (affinity_high, affinity_low, concentration_range,\n",
    "                original_affinity_relation, action_comment, assay_conditions, object_id,\n",
    "                original_affinity_units, endogenous, type_vector, original_affinity_high_nm,\n",
    "                action, affinity_median, voltage_dependent, assay_description, hide,\n",
    "                from_grac, whole_organism_assay, only_grac, original_affinity_low_nm,\n",
    "                percent_activity, affinity_units, affinity_voltage_median, primary_target, selectivity,\n",
    "                use_dependent, species_id, selective, ligand_id, target_ligand_id, ligand_context,\n",
    "                affinity_voltage_high, receptor_site, affinity_voltage_low, assay_url,\n",
    "                original_affinity_median_nm, interaction_id, rank, affinity_physiological_voltage, type.\n",
    "\n",
    "                Much more tables like this exist in the database.\n",
    "                \n",
    "                Here are some examples of how to translate natural language queries into SQL queries:\n",
    "                {examples}\n",
    "                Please write an SQL query based on the following natural language text:\n",
    "            '''},\n",
    "            {\"role\": \"user\", \"content\": question}\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    # Send the request to OpenAI API\n",
    "    response = requests.post(\"https://api.openai.com/v1/chat/completions\", headers=headers, json=data)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        try:\n",
    "            message_content = response.json()['choices'][0]['message']['content']\n",
    "            if '```sql' in message_content:\n",
    "                sql_query = message_content.split('```sql')[1].split('```')[0].strip()\n",
    "                return sql_query\n",
    "            else:\n",
    "                print(\"No SQL query found in the response. Full response:\")\n",
    "                print(message_content)\n",
    "                return None\n",
    "        except Exception as e:\n",
    "            print(f\"Error extracting SQL query: {e}\")\n",
    "            return None\n",
    "    else:\n",
    "        print(f\"Error: {response.status_code} - {response.text}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function to execute query and return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to execute a query and return results\n",
    "def execute_query(conn, sql_query):\n",
    "    \"\"\" Executes a SQL query and returns the result as a DataFrame. \"\"\"\n",
    "    try:\n",
    "        return pd.read_sql_query(sql_query, conn)\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error executing SQL query: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to split the dataset\n",
    "def split_dataset(file_path):\n",
    "    df = pd.read_csv(file_path)\n",
    "    difficulty_cols = [\"Difficulty: Easy\", \"Difficulty: Easy-Moderate\", \"Difficulty: Moderate-Hard\", \"Difficulty: Hard\"]\n",
    "    df[\"Difficulty\"] = df[difficulty_cols].idxmax(axis=1).str.replace(\"Difficulty: \", \"\")\n",
    "    train_df, test_df = train_test_split(df, test_size=0.2, stratify=df[\"Difficulty\"], random_state=42)\n",
    "    return train_df, test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset split into training and testing sets.\n"
     ]
    }
   ],
   "source": [
    "file_path = \"Training/all_queries_categorised_train.csv\"  # Path to your dataset\n",
    "train_df, test_df = split_dataset(file_path)\n",
    "print(\"Dataset split into training and testing sets.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate Query Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to evaluate the query results\n",
    "def evaluate_query(expected_df, result_df):\n",
    "    \"\"\" Evaluates the generated SQL query against the expected result. \"\"\"\n",
    "    \n",
    "    # Get columns that exist in both the expected and result DataFrames\n",
    "    common_columns = set(expected_df.columns).intersection(result_df.columns)\n",
    "    \n",
    "    missed_columns = len(set(expected_df.columns) - set(result_df.columns))\n",
    "    extra_columns = len(set(result_df.columns) - set(expected_df.columns))\n",
    "    \n",
    "    # Calculate precision and recall\n",
    "    common_columns = list(common_columns)\n",
    "    precision = (expected_df[common_columns] == result_df[common_columns]).mean().mean()\n",
    "\n",
    "    # NIKITA\n",
    "    print(\"Expected DF Columns:\", expected_df.columns)\n",
    "    print(\"Result DF Columns:\", result_df.columns)\n",
    "    print(\"Common Columns:\", common_columns)\n",
    "    print(\"Expected DF Index:\", expected_df.index)\n",
    "    print(\"Result DF Index:\", result_df.index)\n",
    "    common_columns = list(set(expected_df.columns) & set(result_df.columns))\n",
    "    print(\"Common Columns After Intersection:\", common_columns)\n",
    "    print(\"Expected DF Shape:\", expected_df.shape)\n",
    "    print(\"Result DF Shape:\", result_df.shape)\n",
    "    # NIKITA\n",
    "\n",
    "    recall = precision  # Assuming recall == precision in this case\n",
    "    \n",
    "    # Calculate accuracy: checks if the DataFrames are identical row-wise and column-wise\n",
    "    accuracy = expected_df.equals(result_df)\n",
    "    \n",
    "    # Calculate row-wise match count\n",
    "    matched_rows = (expected_df == result_df).all(axis=1).sum()\n",
    "    total_rows = len(expected_df)\n",
    "    \n",
    "    # Missed and extra columns are already calculated above\n",
    "    return {\n",
    "        \"Precision\": precision,\n",
    "        \"Recall\": recall,\n",
    "        \"Accuracy\": accuracy,\n",
    "        \"Missed Columns\": missed_columns,\n",
    "        \"Extra Columns\": extra_columns,\n",
    "        \"Matched Rows\": matched_rows,\n",
    "        \"Total Rows\": total_rows\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"Evaluating the test dataset...\")\n",
    "# test_metrics = evaluate_dataset(test_df, conn)\n",
    "# print(\"Evaluation results:\")\n",
    "# print(test_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate SQL for test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sql_queries_for_test_df(test_df):\n",
    "    generated_queries = []\n",
    "    \n",
    "    for _, row in test_df.iterrows():\n",
    "        nl_query = row['Natural Language Query']  # Natural language query\n",
    "        # Generate SQL from OpenAI\n",
    "        generated_sql = ask_openai(nl_query)\n",
    "        \n",
    "        # Add generated SQL to the row for later comparison\n",
    "        row['Generated SQL'] = generated_sql\n",
    "        generated_queries.append(row)\n",
    "    \n",
    "    return pd.DataFrame(generated_queries)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_sql_queries(test_df):\n",
    "    results = []\n",
    "    \n",
    "    for _, row in test_df.iterrows():\n",
    "        expected_sql = row['SQL']\n",
    "        generated_sql = row['Generated SQL']\n",
    "        \n",
    "        # Compare exact match\n",
    "        match = expected_sql == generated_sql\n",
    "        \n",
    "        results.append({\n",
    "            \"ID\": row[\"ID\"],\n",
    "            \"Natural Language Query\": row[\"Natural Language Query\"],\n",
    "            \"Expected SQL\": expected_sql,\n",
    "            \"Generated SQL\": generated_sql,\n",
    "            \"Match\": match\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_sql_queries(test_df, conn):\n",
    "    metrics = []\n",
    "    \n",
    "    for _, row in test_df.iterrows():\n",
    "        sql_query = row[\"SQL\"]\n",
    "        second_sql = row.get(\"2nd SQL\")\n",
    "        \n",
    "        # Attempt to execute the main SQL query\n",
    "        expected_df = execute_query(conn, sql_query)\n",
    "        result_df = execute_query(conn, sql_query)\n",
    "\n",
    "        # If either query failed, log and continue\n",
    "        if expected_df is None or result_df is None:\n",
    "            logging.error(f\"Failed to execute main query for Query ID {row['ID']}: {sql_query}\")\n",
    "            continue\n",
    "\n",
    "        # Evaluate the main query\n",
    "        main_eval = evaluate_query(expected_df, result_df)\n",
    "\n",
    "        if second_sql and pd.notnull(second_sql):\n",
    "            # Attempt to execute the second SQL query\n",
    "            second_expected_df = execute_query(conn, second_sql)\n",
    "            if second_expected_df is None:\n",
    "                logging.error(f\"Failed to execute second query for Query ID {row['ID']}: {second_sql}\")\n",
    "                second_eval = {key: 0 for key in main_eval.keys()}  # Default to 0 for all metrics\n",
    "            else:\n",
    "                second_eval = evaluate_query(second_expected_df, result_df)\n",
    "            \n",
    "            # Combine evaluations (use max values for each metric)\n",
    "            for key in main_eval:\n",
    "                main_eval[key] = max(main_eval[key], second_eval[key])\n",
    "\n",
    "        # Append results to metrics list\n",
    "        metrics.append({\n",
    "            \"Query ID\": row[\"ID\"],\n",
    "            \"Precision\": main_eval[\"Precision\"],\n",
    "            \"Recall\": main_eval[\"Recall\"],\n",
    "            \"Accuracy\": main_eval[\"Accuracy\"],\n",
    "            \"Missed Columns\": main_eval[\"Missed Columns\"],\n",
    "            \"Extra Columns\": main_eval[\"Extra Columns\"],\n",
    "            \"Matched Rows\": main_eval[\"Matched Rows\"],\n",
    "            \"Total Rows\": main_eval[\"Total Rows\"]\n",
    "        })\n",
    "\n",
    "    return pd.DataFrame(metrics)\n",
    "\n",
    "# Example usage:\n",
    "# Assuming `conn` is your active database connection and `test_df` is your test DataFrame.\n",
    "# metrics_df = evaluate_sql_queries(test_df, conn)\n",
    "# print(metrics_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Putting it all together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_full_evaluation(file_path, conn):\n",
    "    # Step 1: Load and split the dataset\n",
    "    train_df, test_df = split_dataset(file_path)\n",
    "\n",
    "    # Step 2: Generate SQL queries for the test set (not the training set)\n",
    "    generated_test_df = generate_sql_queries_for_test_df(test_df)\n",
    "\n",
    "    # Step 3: Compare generated SQL with expected SQL\n",
    "    comparison_df = compare_sql_queries(generated_test_df)\n",
    "    print(\"Comparison of generated and expected SQL queries on the test set:\")\n",
    "    print(comparison_df)\n",
    "\n",
    "    # Step 4: Evaluate SQL queries on the database (for the test set)\n",
    "    evaluation_results = evaluate_sql_queries(generated_test_df, conn)\n",
    "    \n",
    "    # Step 5: Print the evaluation results in a detailed table format\n",
    "    print(\"Detailed Evaluation Results on the test set:\")\n",
    "    print(evaluation_results)\n",
    "\n",
    "    # Optionally, save the results to a file for further analysis\n",
    "    evaluation_results.to_csv('evaluation_results.csv', index=False)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No SQL query found in the response. Full response:\n",
      "This type of question is better suited for searching in a literature database or a digital library, rather than a structured database containing tables about specific data entities. You might consider looking for reviews or research papers in medical and pharmacological journals. If you have access to a database that stores document references, you might search for papers or reviews there using keywords related to endothelin receptors.\n",
      "Comparison of generated and expected SQL queries on the test set:\n",
      "    ID                             Natural Language Query  \\\n",
      "0   65  Is there recommended background reading on end...   \n",
      "1   69  What ligands and structures have a pKi values ...   \n",
      "2   29  What compounds have a role in treating arthritis?   \n",
      "3   16  What WHO essential medicines, that are approve...   \n",
      "4   15  What drugs might be used to treat multiple scl...   \n",
      "5   58  Find information on the clinical use of drugs ...   \n",
      "6   61  Are there any compounds that only target one m...   \n",
      "7   56  Find any endogenous substrates of decarboxylases?   \n",
      "8   42  List ligands for SARS Cov2 MPro that have pAct...   \n",
      "9   23  Find all the papers in the Guide that are list...   \n",
      "10  38  How many antibodies with binding data are ther...   \n",
      "\n",
      "                                         Expected SQL  \\\n",
      "0   select r.reference_id, r.title, r.article_titl...   \n",
      "1   select l.ligand_id, l.name, l.type, ls.isomeri...   \n",
      "2   select idl.ligand_id, l.name, idl.comment, d.n...   \n",
      "3   select ligand_id, name from ligand where who_e...   \n",
      "4   select idl.ligand_id, l.name, idl.comment, d.n...   \n",
      "5   select distinct l.ligand_id, l.name, l.type, l...   \n",
      "6   select i.ligand_id, l.name, count(distinct i.o...   \n",
      "7   select l.ligand_id, l.name, l.type, o.object_i...   \n",
      "8   select l.ligand_id, l.name, l.type, o.object_i...   \n",
      "9   select * from reference where pub_status ilike...   \n",
      "10  select count(distinct l.ligand_id) from intera...   \n",
      "\n",
      "                                        Generated SQL  Match  \n",
      "0                                                None  False  \n",
      "1   SELECT DISTINCT l.name AS ligand_name, l.iupac...  False  \n",
      "2   SELECT DISTINCT ligand_id, name, clinical_use\\...  False  \n",
      "3   SELECT name \\nFROM ligand \\nWHERE who_essentia...  False  \n",
      "4   SELECT name, clinical_use, mechanism_of_action...  False  \n",
      "5   SELECT \\n    l.name AS drug_name,\\n    l.clini...  False  \n",
      "6   SELECT DISTINCT l.name\\nFROM interaction i\\nJO...  False  \n",
      "7   SELECT i.ligand_id, i.object_id, o.name AS enz...  False  \n",
      "8   SELECT l.name\\nFROM interaction i\\nJOIN ligand...  False  \n",
      "9     SELECT * FROM papers WHERE status = 'preprint';  False  \n",
      "10  SELECT COUNT(DISTINCT ligand.ligand_id) AS ant...  False  \n",
      "Expected DF Columns: Index(['reference_id', 'title', 'article_title', 'year', 'pubmed_id',\n",
      "       'key_ref'],\n",
      "      dtype='object')\n",
      "Result DF Columns: Index(['reference_id', 'title', 'article_title', 'year', 'pubmed_id',\n",
      "       'key_ref'],\n",
      "      dtype='object')\n",
      "Common Columns: ['reference_id', 'key_ref', 'year', 'article_title', 'title', 'pubmed_id']\n",
      "Expected DF Index: RangeIndex(start=0, stop=23, step=1)\n",
      "Result DF Index: RangeIndex(start=0, stop=23, step=1)\n",
      "Common Columns After Intersection: ['reference_id', 'key_ref', 'year', 'article_title', 'title', 'pubmed_id']\n",
      "Expected DF Shape: (23, 6)\n",
      "Result DF Shape: (23, 6)\n",
      "Expected DF Columns: Index(['ligand_id', 'name', 'type', 'isomeric_smiles',\n",
      "       'isomeric_standard_inchi_key', 'type', 'action', 'affinity',\n",
      "       'affinity_units'],\n",
      "      dtype='object')\n",
      "Result DF Columns: Index(['ligand_id', 'name', 'type', 'isomeric_smiles',\n",
      "       'isomeric_standard_inchi_key', 'type', 'action', 'affinity',\n",
      "       'affinity_units'],\n",
      "      dtype='object')\n",
      "Common Columns: ['type', 'action', 'ligand_id', 'name', 'affinity_units', 'affinity', 'isomeric_smiles', 'isomeric_standard_inchi_key']\n",
      "Expected DF Index: RangeIndex(start=0, stop=91, step=1)\n",
      "Result DF Index: RangeIndex(start=0, stop=91, step=1)\n",
      "Common Columns After Intersection: ['type', 'action', 'ligand_id', 'name', 'affinity_units', 'affinity', 'isomeric_smiles', 'isomeric_standard_inchi_key']\n",
      "Expected DF Shape: (91, 9)\n",
      "Result DF Shape: (91, 9)\n",
      "Expected DF Columns: Index(['ligand_id', 'name', 'comment', 'name'], dtype='object')\n",
      "Result DF Columns: Index(['ligand_id', 'name', 'comment', 'name'], dtype='object')\n",
      "Common Columns: ['comment', 'ligand_id', 'name']\n",
      "Expected DF Index: RangeIndex(start=0, stop=179, step=1)\n",
      "Result DF Index: RangeIndex(start=0, stop=179, step=1)\n",
      "Common Columns After Intersection: ['comment', 'ligand_id', 'name']\n",
      "Expected DF Shape: (179, 4)\n",
      "Result DF Shape: (179, 4)\n",
      "Expected DF Columns: Index(['ligand_id', 'name'], dtype='object')\n",
      "Result DF Columns: Index(['ligand_id', 'name'], dtype='object')\n",
      "Common Columns: ['ligand_id', 'name']\n",
      "Expected DF Index: RangeIndex(start=0, stop=8, step=1)\n",
      "Result DF Index: RangeIndex(start=0, stop=8, step=1)\n",
      "Common Columns After Intersection: ['ligand_id', 'name']\n",
      "Expected DF Shape: (8, 2)\n",
      "Result DF Shape: (8, 2)\n",
      "Expected DF Columns: Index(['ligand_id', 'name', 'comment', 'name'], dtype='object')\n",
      "Result DF Columns: Index(['ligand_id', 'name', 'comment', 'name'], dtype='object')\n",
      "Common Columns: ['comment', 'ligand_id', 'name']\n",
      "Expected DF Index: RangeIndex(start=0, stop=19, step=1)\n",
      "Result DF Index: RangeIndex(start=0, stop=19, step=1)\n",
      "Common Columns After Intersection: ['comment', 'ligand_id', 'name']\n",
      "Expected DF Shape: (19, 4)\n",
      "Result DF Shape: (19, 4)\n",
      "Expected DF Columns: Index(['ligand_id', 'name', 'type', 'clinical_use'], dtype='object')\n",
      "Result DF Columns: Index(['ligand_id', 'name', 'type', 'clinical_use'], dtype='object')\n",
      "Common Columns: ['type', 'ligand_id', 'name', 'clinical_use']\n",
      "Expected DF Index: RangeIndex(start=0, stop=102, step=1)\n",
      "Result DF Index: RangeIndex(start=0, stop=102, step=1)\n",
      "Common Columns After Intersection: ['type', 'ligand_id', 'name', 'clinical_use']\n",
      "Expected DF Shape: (102, 4)\n",
      "Result DF Shape: (102, 4)\n",
      "Expected DF Columns: Index(['ligand_id', 'name', 'targets'], dtype='object')\n",
      "Result DF Columns: Index(['ligand_id', 'name', 'targets'], dtype='object')\n",
      "Common Columns: ['targets', 'ligand_id', 'name']\n",
      "Expected DF Index: RangeIndex(start=0, stop=2, step=1)\n",
      "Result DF Index: RangeIndex(start=0, stop=2, step=1)\n",
      "Common Columns After Intersection: ['ligand_id', 'targets', 'name']\n",
      "Expected DF Shape: (2, 3)\n",
      "Result DF Shape: (2, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nikit\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\pandas\\io\\sql.py:761: UserWarning: pandas only support SQLAlchemy connectable(engine/connection) ordatabase string URI or sqlite3 DBAPI2 connectionother DBAPI2 objects are not tested, please consider using SQLAlchemy\n",
      "  warnings.warn(\n",
      "C:\\Users\\nikit\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\pandas\\io\\sql.py:761: UserWarning: pandas only support SQLAlchemy connectable(engine/connection) ordatabase string URI or sqlite3 DBAPI2 connectionother DBAPI2 objects are not tested, please consider using SQLAlchemy\n",
      "  warnings.warn(\n",
      "C:\\Users\\nikit\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\pandas\\io\\sql.py:761: UserWarning: pandas only support SQLAlchemy connectable(engine/connection) ordatabase string URI or sqlite3 DBAPI2 connectionother DBAPI2 objects are not tested, please consider using SQLAlchemy\n",
      "  warnings.warn(\n",
      "C:\\Users\\nikit\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\pandas\\io\\sql.py:761: UserWarning: pandas only support SQLAlchemy connectable(engine/connection) ordatabase string URI or sqlite3 DBAPI2 connectionother DBAPI2 objects are not tested, please consider using SQLAlchemy\n",
      "  warnings.warn(\n",
      "C:\\Users\\nikit\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\pandas\\io\\sql.py:761: UserWarning: pandas only support SQLAlchemy connectable(engine/connection) ordatabase string URI or sqlite3 DBAPI2 connectionother DBAPI2 objects are not tested, please consider using SQLAlchemy\n",
      "  warnings.warn(\n",
      "C:\\Users\\nikit\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\pandas\\io\\sql.py:761: UserWarning: pandas only support SQLAlchemy connectable(engine/connection) ordatabase string URI or sqlite3 DBAPI2 connectionother DBAPI2 objects are not tested, please consider using SQLAlchemy\n",
      "  warnings.warn(\n",
      "C:\\Users\\nikit\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\pandas\\io\\sql.py:761: UserWarning: pandas only support SQLAlchemy connectable(engine/connection) ordatabase string URI or sqlite3 DBAPI2 connectionother DBAPI2 objects are not tested, please consider using SQLAlchemy\n",
      "  warnings.warn(\n",
      "C:\\Users\\nikit\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\pandas\\io\\sql.py:761: UserWarning: pandas only support SQLAlchemy connectable(engine/connection) ordatabase string URI or sqlite3 DBAPI2 connectionother DBAPI2 objects are not tested, please consider using SQLAlchemy\n",
      "  warnings.warn(\n",
      "C:\\Users\\nikit\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\pandas\\io\\sql.py:761: UserWarning: pandas only support SQLAlchemy connectable(engine/connection) ordatabase string URI or sqlite3 DBAPI2 connectionother DBAPI2 objects are not tested, please consider using SQLAlchemy\n",
      "  warnings.warn(\n",
      "C:\\Users\\nikit\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\pandas\\io\\sql.py:761: UserWarning: pandas only support SQLAlchemy connectable(engine/connection) ordatabase string URI or sqlite3 DBAPI2 connectionother DBAPI2 objects are not tested, please consider using SQLAlchemy\n",
      "  warnings.warn(\n",
      "C:\\Users\\nikit\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\pandas\\io\\sql.py:761: UserWarning: pandas only support SQLAlchemy connectable(engine/connection) ordatabase string URI or sqlite3 DBAPI2 connectionother DBAPI2 objects are not tested, please consider using SQLAlchemy\n",
      "  warnings.warn(\n",
      "C:\\Users\\nikit\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\pandas\\io\\sql.py:761: UserWarning: pandas only support SQLAlchemy connectable(engine/connection) ordatabase string URI or sqlite3 DBAPI2 connectionother DBAPI2 objects are not tested, please consider using SQLAlchemy\n",
      "  warnings.warn(\n",
      "C:\\Users\\nikit\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\pandas\\io\\sql.py:761: UserWarning: pandas only support SQLAlchemy connectable(engine/connection) ordatabase string URI or sqlite3 DBAPI2 connectionother DBAPI2 objects are not tested, please consider using SQLAlchemy\n",
      "  warnings.warn(\n",
      "C:\\Users\\nikit\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\pandas\\io\\sql.py:761: UserWarning: pandas only support SQLAlchemy connectable(engine/connection) ordatabase string URI or sqlite3 DBAPI2 connectionother DBAPI2 objects are not tested, please consider using SQLAlchemy\n",
      "  warnings.warn(\n",
      "C:\\Users\\nikit\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\pandas\\io\\sql.py:761: UserWarning: pandas only support SQLAlchemy connectable(engine/connection) ordatabase string URI or sqlite3 DBAPI2 connectionother DBAPI2 objects are not tested, please consider using SQLAlchemy\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Can only compare identically-labeled DataFrame objects",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[183], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m file_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining/all_queries_categorised_train.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m----> 2\u001b[0m \u001b[43mrun_full_evaluation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[182], line 14\u001b[0m, in \u001b[0;36mrun_full_evaluation\u001b[1;34m(file_path, conn)\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(comparison_df)\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# Step 4: Evaluate SQL queries on the database (for the test set)\u001b[39;00m\n\u001b[1;32m---> 14\u001b[0m evaluation_results \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate_sql_queries\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgenerated_test_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# Step 5: Print the evaluation results in a detailed table format\u001b[39;00m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDetailed Evaluation Results on the test set:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[181], line 27\u001b[0m, in \u001b[0;36mevaluate_sql_queries\u001b[1;34m(test_df, conn)\u001b[0m\n\u001b[0;32m     25\u001b[0m     second_eval \u001b[38;5;241m=\u001b[39m {key: \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m main_eval\u001b[38;5;241m.\u001b[39mkeys()}  \u001b[38;5;66;03m# Default to 0 for all metrics\u001b[39;00m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 27\u001b[0m     second_eval \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate_query\u001b[49m\u001b[43m(\u001b[49m\u001b[43msecond_expected_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresult_df\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;66;03m# Combine evaluations (use max values for each metric)\u001b[39;00m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m main_eval:\n",
      "Cell \u001b[1;32mIn[177], line 13\u001b[0m, in \u001b[0;36mevaluate_query\u001b[1;34m(expected_df, result_df)\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# Calculate precision and recall\u001b[39;00m\n\u001b[0;32m     12\u001b[0m common_columns \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(common_columns)\n\u001b[1;32m---> 13\u001b[0m precision \u001b[38;5;241m=\u001b[39m (\u001b[43mexpected_df\u001b[49m\u001b[43m[\u001b[49m\u001b[43mcommon_columns\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mresult_df\u001b[49m\u001b[43m[\u001b[49m\u001b[43mcommon_columns\u001b[49m\u001b[43m]\u001b[49m)\u001b[38;5;241m.\u001b[39mmean()\u001b[38;5;241m.\u001b[39mmean()\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# NIKITA\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected DF Columns:\u001b[39m\u001b[38;5;124m\"\u001b[39m, expected_df\u001b[38;5;241m.\u001b[39mcolumns)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\pandas\\core\\ops\\common.py:70\u001b[0m, in \u001b[0;36m_unpack_zerodim_and_defer.<locals>.new_method\u001b[1;34m(self, other)\u001b[0m\n\u001b[0;32m     66\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mNotImplemented\u001b[39m\n\u001b[0;32m     68\u001b[0m other \u001b[38;5;241m=\u001b[39m item_from_zerodim(other)\n\u001b[1;32m---> 70\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmethod\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mother\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\pandas\\core\\arraylike.py:40\u001b[0m, in \u001b[0;36mOpsMixin.__eq__\u001b[1;34m(self, other)\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[38;5;129m@unpack_zerodim_and_defer\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__eq__\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     39\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__eq__\u001b[39m(\u001b[38;5;28mself\u001b[39m, other):\n\u001b[1;32m---> 40\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cmp_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mother\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moperator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meq\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\pandas\\core\\frame.py:6931\u001b[0m, in \u001b[0;36mDataFrame._cmp_method\u001b[1;34m(self, other, op)\u001b[0m\n\u001b[0;32m   6928\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_cmp_method\u001b[39m(\u001b[38;5;28mself\u001b[39m, other, op):\n\u001b[0;32m   6929\u001b[0m     axis \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m  \u001b[38;5;66;03m# only relevant for Series other case\u001b[39;00m\n\u001b[1;32m-> 6931\u001b[0m     \u001b[38;5;28mself\u001b[39m, other \u001b[38;5;241m=\u001b[39m \u001b[43mops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43malign_method_FRAME\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mother\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m   6933\u001b[0m     \u001b[38;5;66;03m# See GH#4537 for discussion of scalar op behavior\u001b[39;00m\n\u001b[0;32m   6934\u001b[0m     new_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dispatch_frame_op(other, op, axis\u001b[38;5;241m=\u001b[39maxis)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\pandas\\core\\ops\\__init__.py:289\u001b[0m, in \u001b[0;36malign_method_FRAME\u001b[1;34m(left, right, axis, flex, level)\u001b[0m\n\u001b[0;32m    287\u001b[0m             left, right \u001b[38;5;241m=\u001b[39m left\u001b[38;5;241m.\u001b[39malign(right, join\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mouter\u001b[39m\u001b[38;5;124m\"\u001b[39m, level\u001b[38;5;241m=\u001b[39mlevel, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m    288\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 289\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    290\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCan only compare identically-labeled DataFrame objects\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    291\u001b[0m             )\n\u001b[0;32m    292\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(right, ABCSeries):\n\u001b[0;32m    293\u001b[0m     \u001b[38;5;66;03m# axis=1 is default for DataFrame-with-Series op\u001b[39;00m\n\u001b[0;32m    294\u001b[0m     axis \u001b[38;5;241m=\u001b[39m left\u001b[38;5;241m.\u001b[39m_get_axis_number(axis) \u001b[38;5;28;01mif\u001b[39;00m axis \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m1\u001b[39m\n",
      "\u001b[1;31mValueError\u001b[0m: Can only compare identically-labeled DataFrame objects"
     ]
    }
   ],
   "source": [
    "file_path = \"Training/all_queries_categorised_train.csv\"\n",
    "run_full_evaluation(file_path, conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# expected_df = expected_df[common_columns]\n",
    "# result_df = result_df[common_columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run your Own Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# while True:\n",
    "#     query = input(\"Enter an SQL query to execute (or type 'exit' to quit): \")\n",
    "#     if query.lower() == 'exit':\n",
    "#         break\n",
    "\n",
    "#     results = execute_query(conn, query)\n",
    "#     if results is not None and not results.empty:\n",
    "#         print(\"Query Results:\")\n",
    "#         print(results)\n",
    "#     else:\n",
    "#         print(\"No results found or an error occurred.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Close DB Connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Database connection closed.\n"
     ]
    }
   ],
   "source": [
    "conn.close()\n",
    "print(\"Database connection closed.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
