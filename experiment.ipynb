{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using LLMs to Explore the IUPHAR Guide to Pharmacology\n",
    "Experiment with LLMs for the IUPHAR/BPS Guide to Pharmacology"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import base64\n",
    "import logging\n",
    "import codecs\n",
    "import requests\n",
    "import psycopg2\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To log any errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(filename=\"query_errors.log\", level=logging.ERROR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Connect to database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "def connect_to_db():\n",
    "    try:\n",
    "        conn = psycopg2.connect(**db_config)\n",
    "        return conn\n",
    "    except Exception as e:\n",
    "        print(f\"Error connecting to the database: {e}\")\n",
    "        return None\n",
    "    \n",
    "# Password generation function\n",
    "def pwd():\n",
    "    s1 = ''.join([chr(int(i)) for i in ['120', '65', '103', '108', '101', '116', '116', '55']])\n",
    "    s2 = base64.b64encode(s1.encode('utf-8')).decode('utf-8')\n",
    "    s3 = codecs.encode(s2[::-1], 'rot_13')\n",
    "    s4 = codecs.decode(s3[::-1], 'rot_13')\n",
    "    return base64.b64decode(s4).decode('utf-8')\n",
    "\n",
    "\n",
    "# Database connection details\n",
    "db_config = {\n",
    "    'host': 'localhost',\n",
    "    'database': 'guide_to_pharmacology',\n",
    "    'user': 'postgres',\n",
    "    'password': pwd(),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to the database successfully.\n"
     ]
    }
   ],
   "source": [
    "conn = connect_to_db()\n",
    "if conn:\n",
    "    print(\"Connected to the database successfully.\")\n",
    "else:\n",
    "    print(\"Failed to connect to the database. Please check your credentials.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function to Train LLM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to convert the training set into OpenAI's prompt-completion format\n",
    "def convert_to_openai_format(df):\n",
    "    openai_format_data = []\n",
    "    for _, row in df.iterrows():\n",
    "        # Format each row as a prompt-completion pair\n",
    "        openai_format_data.append({\n",
    "            \"prompt\": f\"{row['question']}\",\n",
    "            \"completion\": row['sql_query']\n",
    "        })\n",
    "    return openai_format_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function to Integrate LLM and SQL training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to ask a question and get SQL using OpenAI API\n",
    "def ask_openai(question, train_df=None):\n",
    "    api_key = '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'\n",
    "    headers = {\n",
    "        \"Content-Type\": \"application/json\",\n",
    "        \"Authorization\": f\"Bearer {api_key}\"\n",
    "    }\n",
    "\n",
    "    # If a training set is provided, convert it to the OpenAI format and append it to the system prompt\n",
    "    if train_df is not None:\n",
    "        training_data = convert_to_openai_format(train_df)\n",
    "        examples = \"\\n\".join([f\"Q: {item['prompt']} A: {item['completion']}\" for item in training_data])\n",
    "    else:\n",
    "        examples = \"\"\n",
    "    \n",
    "    # Constructing the data payload for OpenAI API with additional examples from the training data\n",
    "    data = {\n",
    "        \"model\": \"gpt-4o\",  # Or use your fine-tuned model if available\n",
    "        \"messages\": [\n",
    "            {\"role\": \"system\", \"content\": f'''\n",
    "                I have a database with the following schema:\n",
    "                Table: ‘object’ with columns: (abbreviation, only_grac, in_cgtp, annotation_status,\n",
    "                comments, object_id, only_iuphar, cite_id, systematic_name, gtip_comment, name,\n",
    "                gtmp_comment, in_gtip, old_object_id, in_gtmp, grac_comments, last_modified,\n",
    "                no_contributor_list, structural_info_comments, quaternary_structure_comments).\n",
    "                \n",
    "                Table: ‘ligand’ with columns: (old_ligand_id, has_chembl_interaction,\n",
    "                name_vector, mechanism_of_action_vector, comments, immuno_comments, antibacterial,\n",
    "                ligand_id, bioactivity_comments, absorption_distribution_vector, pubchem_sid,\n",
    "                metabolism_vector, comments_vector, popn_pharmacokinetics, bioactivity_comments_vector, elimination, drugs_url, abbreviation,\n",
    "                absorption_distribution, organ_function_impairment, emc_url, verified, mechanism_of_action, approved_source, who_essential, abbreviation_vector,\n",
    "                in_gtmp, metabolism, type, elimination_vector, approved, in_gtip, labelled, withdrawn_drug,\n",
    "                name, immuno_comments_vector, popn_pharmacokinetics_vector, ema_url, iupac_name,\n",
    "                clinical_use_vector, gtmp_comments, radioactive, has_qi_interaction,\n",
    "                gtmp_comments_vector, organ_function_impairment_vector, clinical_use).\n",
    "                \n",
    "                Table: ‘interaction’ with columns: (affinity_high, affinity_low, concentration_range,\n",
    "                original_affinity_relation, action_comment, assay_conditions, object_id,\n",
    "                original_affinity_units, endogenous, type_vector, original_affinity_high_nm,\n",
    "                action, affinity_median, voltage_dependent, assay_description, hide,\n",
    "                from_grac, whole_organism_assay, only_grac, original_affinity_low_nm,\n",
    "                percent_activity, affinity_units, affinity_voltage_median, primary_target, selectivity,\n",
    "                use_dependent, species_id, selective, ligand_id, target_ligand_id, ligand_context,\n",
    "                affinity_voltage_high, receptor_site, affinity_voltage_low, assay_url,\n",
    "                original_affinity_median_nm, interaction_id, rank, affinity_physiological_voltage, type.\n",
    "\n",
    "                Much more tables like this exist in the database.\n",
    "                \n",
    "                Here are some examples of how to translate natural language queries into SQL queries:\n",
    "                {examples}\n",
    "                Please write an SQL query based on the following natural language text:\n",
    "            '''},\n",
    "            {\"role\": \"user\", \"content\": question}\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    # Send the request to OpenAI API\n",
    "    response = requests.post(\"https://api.openai.com/v1/chat/completions\", headers=headers, json=data)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        try:\n",
    "            message_content = response.json()['choices'][0]['message']['content']\n",
    "            if '```sql' in message_content:\n",
    "                sql_query = message_content.split('```sql')[1].split('```')[0].strip()\n",
    "                return sql_query\n",
    "            else:\n",
    "                print(\"No SQL query found in the response. Full response:\")\n",
    "                print(message_content)\n",
    "                return None\n",
    "        except Exception as e:\n",
    "            print(f\"Error extracting SQL query: {e}\")\n",
    "            return None\n",
    "    else:\n",
    "        print(f\"Error: {response.status_code} - {response.text}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function to execute query and return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to execute a query and return results\n",
    "def execute_query(conn, sql_query):\n",
    "    \"\"\" Executes a SQL query and returns the result as a DataFrame. \"\"\"\n",
    "    try:\n",
    "        return pd.read_sql_query(sql_query, conn)\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error executing SQL query: {e}\")\n",
    "        return pd.DataFrame()  # Return an empty DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to split the dataset\n",
    "def split_dataset(file_path):\n",
    "    df = pd.read_csv(file_path)\n",
    "    difficulty_cols = [\"Difficulty: Easy\", \"Difficulty: Easy-Moderate\", \"Difficulty: Moderate-Hard\", \"Difficulty: Hard\"]\n",
    "    df[\"Difficulty\"] = df[difficulty_cols].idxmax(axis=1).str.replace(\"Difficulty: \", \"\")\n",
    "    train_df, test_df = train_test_split(df, test_size=0.2, stratify=df[\"Difficulty\"], random_state=42)\n",
    "    return train_df, test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset split into training and testing sets.\n"
     ]
    }
   ],
   "source": [
    "file_path = \"Training/all_queries_categorised_train.csv\"  # Path to your dataset\n",
    "train_df, test_df = split_dataset(file_path)\n",
    "print(\"Dataset split into training and testing sets.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate Query Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to evaluate the query results\n",
    "def evaluate_query(expected_df, result_df):\n",
    "    \"\"\" Evaluates the generated SQL query against the expected result. \"\"\"\n",
    "    \n",
    "    # Get columns that exist in both the expected and result DataFrames\n",
    "    common_columns = set(expected_df.columns).intersection(result_df.columns)\n",
    "    \n",
    "    missed_columns = len(set(expected_df.columns) - set(result_df.columns))\n",
    "    extra_columns = len(set(result_df.columns) - set(expected_df.columns))\n",
    "    \n",
    "    # Calculate precision and recall\n",
    "    common_columns = list(common_columns)\n",
    "    precision = (expected_df[common_columns] == result_df[common_columns]).mean().mean()\n",
    "\n",
    "    recall = precision  # Assuming recall == precision in this case\n",
    "    \n",
    "    # Calculate accuracy: checks if the DataFrames are identical row-wise and column-wise\n",
    "    accuracy = expected_df.equals(result_df)\n",
    "    \n",
    "    # Calculate row-wise match count\n",
    "    matched_rows = (expected_df == result_df).all(axis=1).sum()\n",
    "    total_rows = len(expected_df)\n",
    "    \n",
    "    # Missed and extra columns are already calculated above\n",
    "    return {\n",
    "        \"Precision\": precision,\n",
    "        \"Recall\": recall,\n",
    "        \"Accuracy\": accuracy,\n",
    "        \"Missed Columns\": missed_columns,\n",
    "        \"Extra Columns\": extra_columns,\n",
    "        \"Matched Rows\": matched_rows,\n",
    "        \"Total Rows\": total_rows\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"Evaluating the test dataset...\")\n",
    "# test_metrics = evaluate_dataset(test_df, conn)\n",
    "# print(\"Evaluation results:\")\n",
    "# print(test_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate SQL for test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sql_queries_for_test_df(test_df):\n",
    "    generated_queries = []\n",
    "    \n",
    "    for _, row in test_df.iterrows():\n",
    "        nl_query = row['Natural Language Query']  # Natural language query\n",
    "        # Generate SQL from OpenAI\n",
    "        generated_sql = ask_openai(nl_query)\n",
    "        \n",
    "        # Add generated SQL to the row for later comparison\n",
    "        row['Generated SQL'] = generated_sql\n",
    "        generated_queries.append(row)\n",
    "    \n",
    "    return pd.DataFrame(generated_queries)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_sql_queries(test_df):\n",
    "    results = []\n",
    "    \n",
    "    for _, row in test_df.iterrows():\n",
    "        expected_sql = row['SQL']\n",
    "        generated_sql = row['Generated SQL']\n",
    "        \n",
    "        # Compare exact match\n",
    "        match = expected_sql == generated_sql\n",
    "        \n",
    "        results.append({\n",
    "            \"ID\": row[\"ID\"],\n",
    "            \"Natural Language Query\": row[\"Natural Language Query\"],\n",
    "            \"Expected SQL\": expected_sql,\n",
    "            \"Generated SQL\": generated_sql,\n",
    "            \"Match\": match\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_sql_queries(test_df, conn):\n",
    "    metrics = []\n",
    "    \n",
    "    for _, row in test_df.iterrows():\n",
    "        sql_query = row[\"SQL\"]\n",
    "        # second_sql = row.get(\"2nd SQL\")\n",
    "        \n",
    "        # Attempt to execute the main SQL query\n",
    "        expected_df = execute_query(conn, sql_query)\n",
    "        result_df = execute_query(conn, sql_query)\n",
    "\n",
    "        # If either query failed, log and continue\n",
    "        if expected_df is None or result_df is None:\n",
    "            logging.error(f\"Failed to execute main query for Query ID {row['ID']}: {sql_query}\")\n",
    "            continue\n",
    "\n",
    "        # Evaluate the main query\n",
    "        main_eval = evaluate_query(expected_df, result_df)\n",
    "\n",
    "        # if second_sql and pd.notnull(second_sql):\n",
    "        #     # Attempt to execute the second SQL query\n",
    "        #     second_expected_df = execute_query(conn, second_sql)\n",
    "        #     if second_expected_df is None:\n",
    "        #         logging.error(f\"Failed to execute second query for Query ID {row['ID']}: {second_sql}\")\n",
    "        #         second_eval = {key: 0 for key in main_eval.keys()}  # Default to 0 for all metrics\n",
    "        #     else:\n",
    "        #         second_eval = evaluate_query(second_expected_df, result_df)\n",
    "            \n",
    "        #     # Combine evaluations (use max values for each metric)\n",
    "        #     for key in main_eval:\n",
    "        #         main_eval[key] = max(main_eval[key], second_eval[key])\n",
    "\n",
    "        # Append results to metrics list\n",
    "        metrics.append({\n",
    "            \"Query ID\": row[\"ID\"],\n",
    "            \"Precision\": main_eval[\"Precision\"],\n",
    "            \"Recall\": main_eval[\"Recall\"],\n",
    "            \"Accuracy\": main_eval[\"Accuracy\"],\n",
    "            \"Missed Columns\": main_eval[\"Missed Columns\"],\n",
    "            \"Extra Columns\": main_eval[\"Extra Columns\"],\n",
    "            \"Matched Rows\": main_eval[\"Matched Rows\"],\n",
    "            \"Total Rows\": main_eval[\"Total Rows\"]\n",
    "        })\n",
    "\n",
    "    return pd.DataFrame(metrics)\n",
    "\n",
    "# Example usage:\n",
    "# Assuming `conn` is your active database connection and `test_df` is your test DataFrame.\n",
    "# metrics_df = evaluate_sql_queries(test_df, conn)\n",
    "# print(metrics_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Putting it all together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_full_evaluation(file_path, conn):\n",
    "    # Step 1: Load and split the dataset\n",
    "    train_df, test_df = split_dataset(file_path)\n",
    "\n",
    "    # Step 2: Generate SQL queries for the test set (not the training set)\n",
    "    generated_test_df = generate_sql_queries_for_test_df(test_df)\n",
    "\n",
    "    # Step 3: Compare generated SQL with expected SQL\n",
    "    comparison_df = compare_sql_queries(generated_test_df)\n",
    "    print(\"Comparison of generated and expected SQL queries on the test set:\")\n",
    "    print(comparison_df)\n",
    "\n",
    "    # Step 4: Evaluate SQL queries on the database (for the test set)\n",
    "    evaluation_results = evaluate_sql_queries(generated_test_df, conn)\n",
    "    \n",
    "    # Step 5: Print the evaluation results in a detailed table format\n",
    "    print(\"Detailed Evaluation Results on the test set:\")\n",
    "    print(evaluation_results)\n",
    "\n",
    "    # Optionally, save the results to a file for further analysis\n",
    "    evaluation_results.to_csv('evaluation_results.csv', index=False)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nikit\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\pandas\\io\\sql.py:761: UserWarning: pandas only support SQLAlchemy connectable(engine/connection) ordatabase string URI or sqlite3 DBAPI2 connectionother DBAPI2 objects are not tested, please consider using SQLAlchemy\n",
      "  warnings.warn(\n",
      "C:\\Users\\nikit\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\pandas\\io\\sql.py:761: UserWarning: pandas only support SQLAlchemy connectable(engine/connection) ordatabase string URI or sqlite3 DBAPI2 connectionother DBAPI2 objects are not tested, please consider using SQLAlchemy\n",
      "  warnings.warn(\n",
      "C:\\Users\\nikit\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\pandas\\io\\sql.py:761: UserWarning: pandas only support SQLAlchemy connectable(engine/connection) ordatabase string URI or sqlite3 DBAPI2 connectionother DBAPI2 objects are not tested, please consider using SQLAlchemy\n",
      "  warnings.warn(\n",
      "C:\\Users\\nikit\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\pandas\\io\\sql.py:761: UserWarning: pandas only support SQLAlchemy connectable(engine/connection) ordatabase string URI or sqlite3 DBAPI2 connectionother DBAPI2 objects are not tested, please consider using SQLAlchemy\n",
      "  warnings.warn(\n",
      "C:\\Users\\nikit\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\pandas\\io\\sql.py:761: UserWarning: pandas only support SQLAlchemy connectable(engine/connection) ordatabase string URI or sqlite3 DBAPI2 connectionother DBAPI2 objects are not tested, please consider using SQLAlchemy\n",
      "  warnings.warn(\n",
      "C:\\Users\\nikit\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\pandas\\io\\sql.py:761: UserWarning: pandas only support SQLAlchemy connectable(engine/connection) ordatabase string URI or sqlite3 DBAPI2 connectionother DBAPI2 objects are not tested, please consider using SQLAlchemy\n",
      "  warnings.warn(\n",
      "C:\\Users\\nikit\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\pandas\\io\\sql.py:761: UserWarning: pandas only support SQLAlchemy connectable(engine/connection) ordatabase string URI or sqlite3 DBAPI2 connectionother DBAPI2 objects are not tested, please consider using SQLAlchemy\n",
      "  warnings.warn(\n",
      "C:\\Users\\nikit\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\pandas\\io\\sql.py:761: UserWarning: pandas only support SQLAlchemy connectable(engine/connection) ordatabase string URI or sqlite3 DBAPI2 connectionother DBAPI2 objects are not tested, please consider using SQLAlchemy\n",
      "  warnings.warn(\n",
      "C:\\Users\\nikit\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\pandas\\io\\sql.py:761: UserWarning: pandas only support SQLAlchemy connectable(engine/connection) ordatabase string URI or sqlite3 DBAPI2 connectionother DBAPI2 objects are not tested, please consider using SQLAlchemy\n",
      "  warnings.warn(\n",
      "C:\\Users\\nikit\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\pandas\\io\\sql.py:761: UserWarning: pandas only support SQLAlchemy connectable(engine/connection) ordatabase string URI or sqlite3 DBAPI2 connectionother DBAPI2 objects are not tested, please consider using SQLAlchemy\n",
      "  warnings.warn(\n",
      "C:\\Users\\nikit\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\pandas\\io\\sql.py:761: UserWarning: pandas only support SQLAlchemy connectable(engine/connection) ordatabase string URI or sqlite3 DBAPI2 connectionother DBAPI2 objects are not tested, please consider using SQLAlchemy\n",
      "  warnings.warn(\n",
      "C:\\Users\\nikit\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\pandas\\io\\sql.py:761: UserWarning: pandas only support SQLAlchemy connectable(engine/connection) ordatabase string URI or sqlite3 DBAPI2 connectionother DBAPI2 objects are not tested, please consider using SQLAlchemy\n",
      "  warnings.warn(\n",
      "C:\\Users\\nikit\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\pandas\\io\\sql.py:761: UserWarning: pandas only support SQLAlchemy connectable(engine/connection) ordatabase string URI or sqlite3 DBAPI2 connectionother DBAPI2 objects are not tested, please consider using SQLAlchemy\n",
      "  warnings.warn(\n",
      "C:\\Users\\nikit\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\pandas\\io\\sql.py:761: UserWarning: pandas only support SQLAlchemy connectable(engine/connection) ordatabase string URI or sqlite3 DBAPI2 connectionother DBAPI2 objects are not tested, please consider using SQLAlchemy\n",
      "  warnings.warn(\n",
      "C:\\Users\\nikit\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\pandas\\io\\sql.py:761: UserWarning: pandas only support SQLAlchemy connectable(engine/connection) ordatabase string URI or sqlite3 DBAPI2 connectionother DBAPI2 objects are not tested, please consider using SQLAlchemy\n",
      "  warnings.warn(\n",
      "C:\\Users\\nikit\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\pandas\\io\\sql.py:761: UserWarning: pandas only support SQLAlchemy connectable(engine/connection) ordatabase string URI or sqlite3 DBAPI2 connectionother DBAPI2 objects are not tested, please consider using SQLAlchemy\n",
      "  warnings.warn(\n",
      "C:\\Users\\nikit\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\pandas\\io\\sql.py:761: UserWarning: pandas only support SQLAlchemy connectable(engine/connection) ordatabase string URI or sqlite3 DBAPI2 connectionother DBAPI2 objects are not tested, please consider using SQLAlchemy\n",
      "  warnings.warn(\n",
      "C:\\Users\\nikit\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\pandas\\io\\sql.py:761: UserWarning: pandas only support SQLAlchemy connectable(engine/connection) ordatabase string URI or sqlite3 DBAPI2 connectionother DBAPI2 objects are not tested, please consider using SQLAlchemy\n",
      "  warnings.warn(\n",
      "C:\\Users\\nikit\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\pandas\\io\\sql.py:761: UserWarning: pandas only support SQLAlchemy connectable(engine/connection) ordatabase string URI or sqlite3 DBAPI2 connectionother DBAPI2 objects are not tested, please consider using SQLAlchemy\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detailed Evaluation Results on the test set:\n",
      "    Query ID  Precision    Recall  Accuracy  Missed Columns  Extra Columns  \\\n",
      "0         65   1.000000  1.000000      True               0              0   \n",
      "1         69   1.000000  1.000000      True               0              0   \n",
      "2         29   1.000000  1.000000      True               0              0   \n",
      "3         16   1.000000  1.000000      True               0              0   \n",
      "4         15   1.000000  1.000000      True               0              0   \n",
      "5         58   0.987745  0.987745      True               0              0   \n",
      "6         61   1.000000  1.000000      True               0              0   \n",
      "7         56   1.000000  1.000000      True               0              0   \n",
      "8         42   1.000000  1.000000      True               0              0   \n",
      "9         23   0.424814  0.424814      True               0              0   \n",
      "10        38   1.000000  1.000000      True               0              0   \n",
      "\n",
      "    Matched Rows  Total Rows  \n",
      "0             23          23  \n",
      "1             91          91  \n",
      "2            179         179  \n",
      "3              8           8  \n",
      "4             19          19  \n",
      "5             97         102  \n",
      "6              2           2  \n",
      "7             14          14  \n",
      "8             93          93  \n",
      "9              0          65  \n",
      "10             1           1  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nikit\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\pandas\\io\\sql.py:761: UserWarning: pandas only support SQLAlchemy connectable(engine/connection) ordatabase string URI or sqlite3 DBAPI2 connectionother DBAPI2 objects are not tested, please consider using SQLAlchemy\n",
      "  warnings.warn(\n",
      "C:\\Users\\nikit\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\pandas\\io\\sql.py:761: UserWarning: pandas only support SQLAlchemy connectable(engine/connection) ordatabase string URI or sqlite3 DBAPI2 connectionother DBAPI2 objects are not tested, please consider using SQLAlchemy\n",
      "  warnings.warn(\n",
      "C:\\Users\\nikit\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\pandas\\io\\sql.py:761: UserWarning: pandas only support SQLAlchemy connectable(engine/connection) ordatabase string URI or sqlite3 DBAPI2 connectionother DBAPI2 objects are not tested, please consider using SQLAlchemy\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "file_path = \"Training/all_queries_categorised_train.csv\"\n",
    "run_full_evaluation(file_path, conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run your Own Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [],
   "source": [
    "# while True:\n",
    "#     query = input(\"Enter an SQL query to execute (or type 'exit' to quit): \")\n",
    "#     if query.lower() == 'exit':\n",
    "#         break\n",
    "\n",
    "#     results = execute_query(conn, query)\n",
    "#     if results is not None and not results.empty:\n",
    "#         print(\"Query Results:\")\n",
    "#         print(results)\n",
    "#     else:\n",
    "#         print(\"No results found or an error occurred.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Close DB Connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Database connection closed.\n"
     ]
    }
   ],
   "source": [
    "conn.close()\n",
    "print(\"Database connection closed.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
